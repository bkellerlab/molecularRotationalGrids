import yaml
import sys
import pandas as pd

sys.path.append(".")

from molgri.paths import PATH_INPUT_BASEGRO, PATH_EXPERIMENTS


# read from database and prepare all experiments
all_experiments = pd.read_csv("workflow/all_paper_2024_experiments.csv")
# grid experiments only
grid_experiments = all_experiments[all_experiments["experiment_type"]=="grids"]
sqra_experiments  = all_experiments[all_experiments["experiment_type"]=="sqra_water_in_vacuum"]
msm_vacuum_experiments = all_experiments[all_experiments["experiment_type"]=="msm_water_in_vacuum"]
msm_helium_experiments = all_experiments[all_experiments["experiment_type"]=="msm_water_in_helium"]

rule all:
    input:
        # config_files_grid = expand(f"{PATH_EXPERIMENTS}{{experiment_type}}/{{grid_id}}/total_benchmark.txt",
        #     experiment_type="grids",
        #     grid_id=grid_experiments["grid_identifier"].to_list()),
        # config_files_sqra_vacuum= expand(f"{PATH_EXPERIMENTS}sqra_water_in_vacuum/{{experiment_id}}/{{grid_id}}/total_benchmark.txt", zip,
        #     experiment_id=sqra_experiments["experiment_id"].to_list(),
        #     grid_id=sqra_experiments["grid_identifier"].to_list()),
        config_files_msm_vacuum = expand(f"{PATH_EXPERIMENTS}msm_water_in_vacuum/{{experiment_id}}/{{grid_id}}/total_benchmark.txt", zip,
            experiment_id=msm_vacuum_experiments["experiment_id"].to_list(),
            grid_id=msm_vacuum_experiments["grid_identifier"].to_list()),
        config_files_msm_helium = expand(f"{PATH_EXPERIMENTS}msm_water_in_helium/{{experiment_id}}/{{grid_id}}/total_benchmark.txt", zip,
            experiment_id=msm_helium_experiments["experiment_id"].to_list(),
            grid_id=msm_helium_experiments["grid_identifier"].to_list()),


rule create_config_only_grid:
    """
    Prepare configuration files to feed into sqra/msm pipeline. This might change but currently means:
    - grids 80_80_very_short and cartesian_80_80_very_short
    - tau_t = 1, 0.1, 0.01 and 0.001 ps
    """
    wildcard_constraints:
        experiment_type=".*grid.*"
    input:
        default_config = f"{PATH_INPUT_BASEGRO}default_configuration_file.yaml"
    output:
        config = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{grid_id}}/config_file.yaml"
    run:
        # prepare grid config files
        my_grid_dict = grid_experiments[grid_experiments["grid_identifier"]==wildcards.grid_id].iloc[0].to_dict()

        with open(input.default_config,'r') as f:
            doc = yaml.safe_load(f)

        # change the keywords
        doc["experiment_type"] = wildcards.experiment_type
        doc["experiment_id"] = "run_grid_only"
        doc["grid_identifier"] = wildcards.grid_id

        # change grid params
        for key, value in my_grid_dict.items():
            if key in doc["params_grid"].keys():
                doc["params_grid"][key] = value

        with open(output.config,"w") as f:
            yaml.dump(doc,f)



rule create_config_all:
    """
    Prepare configuration files to feed into sqra/msm pipeline. This might change but currently means:
    - grids 80_80_very_short and cartesian_80_80_very_short
    - tau_t = 1, 0.1, 0.01 and 0.001 ps
    """
    input:
        default_config = f"{PATH_INPUT_BASEGRO}default_configuration_file.yaml"
    output:
        config = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_id}}/config_file.yaml"
    run:
        # prepare grid config files
        my_grid_dict = grid_experiments[grid_experiments["grid_identifier"] == wildcards.grid_id].iloc[0].to_dict()

        if wildcards.experiment_type == "sqra_water_in_vacuum":
            my_experiment_dict = sqra_experiments[sqra_experiments["experiment_id"] == wildcards.experiment_id].iloc[0].to_dict()
        elif wildcards.experiment_type == "msm_water_in_vacuum":
            my_experiment_dict = msm_vacuum_experiments[msm_vacuum_experiments["experiment_id"] == wildcards.experiment_id].iloc[0].to_dict()
        elif wildcards.experiment_type == "msm_water_in_helium":
            my_experiment_dict = msm_helium_experiments[msm_helium_experiments["experiment_id"] == wildcards.experiment_id].iloc[0].to_dict()
        else:
            raise ValueError(f"Don't know what configs to try for experiment type {wildcards.experiment_type}")

        with open(input.default_config,'r') as f:
            doc = yaml.safe_load(f)

        # change the keywords
        doc["experiment_id"] = wildcards.experiment_id
        doc["experiment_type"] = wildcards.experiment_type
        doc["grid_identifier"] = wildcards.grid_id

        # change grid params
        for key, value in my_grid_dict.items():
            if key in doc["params_grid"].keys():
                doc["params_grid"][key] = value

        # change setup params
        for key, value in my_experiment_dict.items():
            if key in doc["params_setup"].keys():
                doc["params_setup"][key] = value

        with open(output.config,"w") as f:
            yaml.dump(doc,f)

# why are the touch rules important? They force the previous config rules always to run and thus update config files.
# Because config files are not directly an input to the rest of the rules, within the modules, only the rules will be
# re-run that depend on parameters that have changed.
rule touch_output:
    input:
        config = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_id}}/config_file.yaml"
    output:
        temp(f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_id}}/touch_config_file.yaml")
    shell:
        "touch {output}"

rule touch_output_grid:
    input:
        config = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{grid_id}}/config_file.yaml"
    output:
        temp(f"{PATH_EXPERIMENTS}{{experiment_type}}/{{grid_id}}/touch_config_file.yaml")
    shell:
        "touch {output}"


rule run_all_grids:
    """
    In the shell run each of configuration set-ups.
    """
    wildcard_constraints:
        experiment_type=".*grid.*"
    input:
        config_file = rules.create_config_only_grid.output.config,
        pipeline = "workflow/run_grid"
    benchmark:
        f"{PATH_EXPERIMENTS}{{experiment_type}}/{{grid_id}}/total_benchmark.txt"
    resources:
        cores = 10
    shell:
        "snakemake --snakefile {input.pipeline} --cores {resources.cores} --configfile {input.config_file}  --rerun-incomplete --keep-going --nolock"

rule run_all_sqra_water_in_vacuum:
    """
    In the shell run each of configuration set-ups.
    """
    wildcard_constraints:
        experiment_type=".*sqra_water_in_vacuum.*"
    input:
        config_file = rules.create_config_all.output.config,
        pipeline = "workflow/run_sqra"
    benchmark:
        f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_id}}/total_benchmark.txt"
    resources:
        cores = 10
    shell:
        """
        snakemake --snakefile {input.pipeline} --cores {resources.cores} --configfile {input.config_file} --rerun-incomplete --keep-going --nolock -F
        """

rule run_all_msm_water_in_vacuum:
    """
    In the shell run each of configuration set-ups.
    """
    wildcard_constraints:
        experiment_type=".*msm_water_in.*"
    input:
        config_file = rules.create_config_all.output.config,
        pipeline = "workflow/run_msm"
    benchmark:
        f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_id}}/total_benchmark.txt"
    resources:
        cores = 10
    shell:
        "snakemake --snakefile {input.pipeline} --cores {resources.cores} --configfile {input.config_file} --rerun-incomplete --keep-going --nolock"
