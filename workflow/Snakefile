import yaml
import sys
import pandas as pd

sys.path.append(".")

import matplotlib.pyplot as plt
plt.switch_backend('agg')

from molgri.paths import PATH_EXPERIMENTS, PATH_EXAMPLES, PATH_INPUT_BASEGRO


# read from database and prepare all experiments
all_experiments = pd.read_csv("workflow/all_paper_2024_experiments.csv")
# grid experiments only
grid_experiments = all_experiments[all_experiments["experiment_type"]=="grids"]
sqra_experiments  = all_experiments[all_experiments["experiment_type"]=="sqra_water_in_vacuum"]
msm_vacuum_experiments = all_experiments[all_experiments["experiment_type"]=="msm_water_in_vacuum"]
msm_helium_experiments = all_experiments[all_experiments["experiment_type"]=="msm_water_in_helium"]

rule all:
    input:
        config_files_grid = expand(f"{PATH_EXPERIMENTS}{{experiment_type}}/{{grid_id}}/total_benchmark.txt",
            experiment_type="grids",
            grid_id=grid_experiments["grid_identifier"].to_list()),
        config_files_sqra_vacuum= expand(f"{PATH_EXPERIMENTS}sqra_water_in_vacuum/{{experiment_id}}/{{grid_id}}/total_benchmark.txt", zip,
            experiment_id=sqra_experiments["experiment_id"].to_list(),
            grid_id=sqra_experiments["grid_identifier"].to_list()),
        config_files_msm_vacuum = expand(f"{PATH_EXPERIMENTS}msm_water_in_vacuum/{{experiment_id}}/{{grid_id}}/total_benchmark.txt", zip,
            experiment_id=msm_vacuum_experiments["experiment_id"].to_list(),
            grid_id=msm_vacuum_experiments["grid_identifier"].to_list()),
        config_files_msm_helium = expand(f"{PATH_EXPERIMENTS}msm_water_in_helium/{{experiment_id}}/{{grid_id}}/total_benchmark.txt", zip,
            experiment_id=msm_helium_experiments["experiment_id"].to_list(),
            grid_id=msm_helium_experiments["grid_identifier"].to_list()),


rule create_config_only_grid:
    """
    Prepare configuration files to feed into sqra/msm pipeline. This might change but currently means:
    - grids 80_80_very_short and cartesian_80_80_very_short
    - tau_t = 1, 0.1, 0.01 and 0.001 ps
    """
    wildcard_constraints:
        experiment_type=".*grid.*"
    input:
        default_config = f"{PATH_EXAMPLES}default_configuration_file.yaml"
    output:
        config = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{grid_id}}/config_file.yaml"
    run:
        # prepare grid config files
        my_grid_dict = grid_experiments[grid_experiments["grid_identifier"]==wildcards.grid_id].iloc[0].to_dict()

        with open(input.default_config,'r') as f:
            doc = yaml.safe_load(f)

        # change the keywords
        doc["experiment_type"] = wildcards.experiment_type
        doc["experiment_id"] = "run_grid_only"
        doc["grid_identifier"] = wildcards.grid_id

        # change grid params
        for key, value in my_grid_dict.items():
            if key in doc["params_grid"].keys():
                doc["params_grid"][key] = value

        with open(output.config,"w") as f:
            yaml.dump(doc,f)



rule create_config_all:
    """
    Prepare configuration files to feed into sqra/msm pipeline. This might change but currently means:
    - grids 80_80_very_short and cartesian_80_80_very_short
    - tau_t = 1, 0.1, 0.01 and 0.001 ps
    """
    input:
        default_config = f"{PATH_EXAMPLES}default_configuration_file.yaml"
    output:
        config = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_id}}/config_file.yaml"
    run:
        # prepare grid config files
        my_grid_dict = grid_experiments[grid_experiments["grid_identifier"] == wildcards.grid_id].iloc[0].to_dict()

        if wildcards.experiment_type == "sqra_water_in_vacuum":
            my_experiment_dict = sqra_experiments[sqra_experiments["experiment_id"] == wildcards.experiment_id].iloc[0].to_dict()
        elif wildcards.experiment_type == "msm_water_in_vacuum":
            my_experiment_dict = msm_vacuum_experiments[msm_vacuum_experiments["experiment_id"] == wildcards.experiment_id].iloc[0].to_dict()
        elif wildcards.experiment_type == "msm_water_in_helium":
            my_experiment_dict = msm_helium_experiments[msm_helium_experiments["experiment_id"] == wildcards.experiment_id].iloc[0].to_dict()
        else:
            raise ValueError(f"Don't know what configs to try for experiment type {wildcards.experiment_type}")

        with open(input.default_config,'r') as f:
            doc = yaml.safe_load(f)

        # change the keywords
        doc["experiment_id"] = wildcards.experiment_id
        doc["experiment_type"] = wildcards.experiment_type
        doc["grid_identifier"] = wildcards.grid_id

        # change grid params
        for key, value in my_grid_dict.items():
            if key in doc["params_grid"].keys():
                doc["params_grid"][key] = value

        # change setup params
        for key, value in my_experiment_dict.items():
            if key in doc["params_setup"].keys():
                doc["params_setup"][key] = value

        with open(output.config,"w") as f:
            yaml.dump(doc,f)

# why are the touch rules important? They force the previous config rules always to run and thus update config files.
# Because config files are not directly an input to the rest of the rules, within the modules, only the rules will be
# re-run that depend on parameters that have changed.
rule touch_output:
    input:
        config = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_id}}/config_file.yaml"
    output:
        temp(f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_id}}/touch_config_file.yaml")
    shell:
        "touch {output}"

rule touch_output_grid:
    input:
        config = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{grid_id}}/config_file.yaml"
    output:
        temp(f"{PATH_EXPERIMENTS}{{experiment_type}}/{{grid_id}}/touch_config_file.yaml")
    shell:
        "touch {output}"


rule run_all_grids:
    """
    In the shell run each of configuration set-ups.
    """
    wildcard_constraints:
        experiment_type=".*grid.*"
    input:
        config_file = rules.create_config_only_grid.output.config,
        pipeline = "workflow/run_grid"
    benchmark:
        f"{PATH_EXPERIMENTS}{{experiment_type}}/{{grid_id}}/total_benchmark.txt"
    resources:
        cores = 10
    shell:
        "snakemake --snakefile {input.pipeline} --cores {resources.cores} --configfile {input.config_file}  --rerun-incomplete --keep-going --nolock"

rule run_all_sqra_water_in_vacuum:
    """
    In the shell run each of configuration set-ups.
    """
    wildcard_constraints:
        experiment_type=".*sqra_water_in_vacuum.*"
    input:
        config_file = rules.create_config_all.output.config,
        pipeline = "workflow/run_sqra"
    benchmark:
        f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_id}}/total_benchmark.txt"
    resources:
        cores = 10
    shell:
        """
        snakemake --snakefile {input.pipeline} --cores {resources.cores} --configfile {input.config_file} --rerun-incomplete --keep-going --nolock -F
        """

rule run_all_msm_water_in_vacuum:
    """
    In the shell run each of configuration set-ups.
    """
    wildcard_constraints:
        experiment_type=".*msm_water_in.*"
    input:
        config_file = rules.create_config_all.output.config,
        pipeline = "workflow/run_msm"
    benchmark:
        f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_id}}/total_benchmark.txt"
    resources:
        cores = 10
    shell:
        "snakemake --snakefile {input.pipeline} --cores {resources.cores} --configfile {input.config_file} --rerun-incomplete --keep-going --nolock"

######################## ALL ABOUT SQRA BENCHMARKING #############################
# run one after the other:
# poetry run snakemake --snakefile workflow/Snakefile run_all_benchmarking_sqra_5repetitions -F --rerun-incomplete
# poetry run snakemake --snakefile workflow/Snakefile plot_all_benchmarking --allowed-rules plot_all_benchmarking combine_benchmarking


rule run_all_benchmarking_sqra_5repetitions:
    """
    Need to run this separately!
    """
    input:
        expand(f"{PATH_EXPERIMENTS}total_benchmarks/{{experiment_type}}_{{which}}_benchmark{{repeats}}.txt",
            experiment_type=["sqra_water_in_vacuum", "sqra_fullerene"], which=["large"],
        repeats = ["_5repeats"]) #"sqra_water_in_vacuum", "sqra_fullerene", "sqra_bpti_trypsine"

rule run_partial_benchmarking_sqra:
    """
    Run a full experiment once but repeat parts of it multiple times so that you can report the time needed for individual rules.
    """
    input:
        config_file=f"{PATH_INPUT_BASEGRO}{{experiment_type}}/all_config_files/default_{{which}}_configuration_file.yaml"
    benchmark:
        f"{PATH_EXPERIMENTS}total_benchmarks/{{experiment_type}}_{{which}}_benchmark_5repeats.txt"
    resources:
        cores = 10
    shell:
        "snakemake --snakefile workflow/run_sqra --cores {resources.cores} --configfile {input.config_file}  --keep-going --rerun-incomplete -F --config num_repeats=5"


rule plot_all_benchmarking:
    input:
        total_statistics = expand(f"{PATH_EXPERIMENTS}{{experiment_type}}/example/{{grid_identifier}}/large_total_statistics.csv", zip,
            experiment_type=["sqra_water_in_vacuum","sqra_fullerene", "sqra_bpti_trypsine"],
        grid_identifier=["large_example_water", "large_example_fullerene", "large_example_bpti"])
    output:
        plot = f"{PATH_EXPERIMENTS}total_benchmarks/large_benchmark_plot.png"
    run:
        import pandas as pd
        import seaborn as sns
        import matplotlib.gridspec as gridspec
        from matplotlib.ticker import ScalarFormatter
        sns.set_context("paper", font_scale=1.5)
        #sns.set_theme("ticks")

        # Create a figure and a GridSpec layout
        fig = plt.figure(figsize=(7, 6))  # Adjust figure size to your needs
        gs = gridspec.GridSpec(3,2,width_ratios=[2, 1], wspace=0.4, hspace=0.5)
        ax1 = fig.add_subplot(gs[1:, 0])  # span all columns in the first row


        plt.tight_layout()  # Adjust layout to prevent overlap
        plt.show()



        all_inputs = []
        for input_file in input:
            all_inputs.append(pd.read_csv(input_file, index_col=0))
        total_df = pd.concat(all_inputs, ignore_index=True)
        total_df["Total time [min]"] = total_df["Total [s]"]/60 /5 # because 5 repetitions
        total_df["Generation of grid+pt [min]"] = total_df["Generation of structures [s]"] / 60 +total_df["Generation of grid [s]"] / 60
        total_df["Force field evaluation [min]"] = total_df["Force field evaluation [s]"] / 60
        total_df["Matrix decomposition [min]"] = total_df["Matrix decomposition [s]"] / 60
        total_df["Cat2"] = total_df["Generation of grid+pt [min]"] +total_df["Force field evaluation [min]"]
        total_df["Cat3"] = total_df["Cat2"] + total_df["Matrix decomposition [min]"]

        # total time stacked bars
        bar1 = sns.barplot(x="Num atoms",y="Total time [min]",data=total_df,color='gray',ax=ax1, width=0.8)
        bar4 = sns.barplot(x="Num atoms",y="Cat3",data=total_df,color='blue',ax=ax1,errorbar=None, width=0.8)
        bar3 = sns.barplot(x="Num atoms",y="Cat2" ,data=total_df,color='green',ax=ax1,errorbar=None, width=0.8)
        bar2 = sns.barplot(x="Num atoms",y="Generation of grid+pt [min]",data=total_df,color='red',ax=ax1,errorbar=None, width=0.8)


        #bar1 = sns.barplot(x="Num atoms",y="Generation of grid + structures [min]",data=grid_average,color='red', ax=ax1)
        # ff_average = total_df.groupby("Num atoms")["Force field evaluation [min]"].mean().reset_index()
        # bar2 = sns.barplot(x="Num atoms",y="Force field evaluation [min]" ,data=ff_average,color='green',ax=ax1)
        # matrix_average = total_df.groupby("Num atoms")["Matrix decomposition [min]"].mean().reset_index()
        # bar3 = sns.barplot(x="Num atoms",y="Matrix decomposition [min]" ,data=matrix_average,color='blue',ax=ax1)


        # rest: scatterplots
        ax2 = fig.add_subplot(gs[0, 1])
        sns.scatterplot(total_df,x="Num atoms",y="Generation of grid+pt [min]",ax=ax2, color="red")
        ax4 = fig.add_subplot(gs[1, 1])
        sns.scatterplot(total_df,x="Num atoms",y="Force field evaluation [min]",ax=ax4, color="green")
        ax5 = fig.add_subplot(gs[2, 1])
        sns.scatterplot(total_df,x="Num atoms",y="Matrix decomposition [min]",ax=ax5, color="blue")

        for myax in [ax2, ax4, ax5]:
            my_label = myax.get_ylabel()
            myax.set(ylabel="", title=my_label, xlabel="")
            myax.set(xscale="log")
            unique_x = sorted(total_df['Num atoms'].unique())  # Get unique x-values and sort them
            myax.xaxis.set_major_formatter(ScalarFormatter())
            myax.xaxis.get_major_formatter().set_scientific(False)
            myax.xaxis.get_major_formatter().set_useOffset(False)
            myax.set_xticks(unique_x)
        for myax in [ax1, ax2, ax4]:
            myax.set(xlabel="")


        fig.tight_layout()
        plt.subplots_adjust(left=0.11,right=0.89,top=0.9,bottom=0.1)
        fig.savefig(output.plot, dpi=600)

rule combine_benchmarking:
    input:
        total_benchmark = f"{PATH_EXPERIMENTS}total_benchmarks/{{experiment_type}}_{{which}}_benchmark_5repeats.txt",
        decomposition_benchmark = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/decomposition_benchmark.txt",
        gromacs_benchmark = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/gromacs_benchmark.txt",
        pt_benchmark = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/pt_benchmark.txt",
        grid_benchmark = f"{PATH_EXPERIMENTS}grids/{{grid_identifier}}/grid_benchmark.txt",
        num_atoms= f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/num_atoms.txt"
    output:
        total_statistics = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/{{which}}_total_statistics.csv"
    run:
        import pandas as pd
        file_to_label = {input.total_benchmark: "Total", input.decomposition_benchmark: "Matrix decomposition",
                         input.gromacs_benchmark: "Force field evaluation", input.pt_benchmark: "Generation of structures",
                         input.grid_benchmark: "Generation of grid"}


        all_input = []
        for input_file in input:
            if "benchmark" in input_file:
                raw_df = pd.read_csv(input_file, delimiter="\t")
                label_s = f"{file_to_label[input_file]} [s]"
                label_hms = f"{file_to_label[input_file]} [h:m:s]"
                raw_df[label_s] = raw_df["s"]
                raw_df[label_hms] = raw_df["h:m:s"]
                clean_df = raw_df[[label_s, label_hms]]
                all_input.append(clean_df)
        total_df = pd.concat(all_input, axis=1)
        with open(input.num_atoms,"r") as f:
            num_first_mol = int(f.readline().strip())
            num_second_mol = int(f.readline().strip())
        total_df["Num atoms"] = num_first_mol + num_second_mol
        total_df.to_csv(output.total_statistics)

