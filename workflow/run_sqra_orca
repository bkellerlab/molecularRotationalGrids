"""Extra functions for sqra pipelines with orca on curta."""
import sys
import os
import subprocess
import numpy as np

sys.path.append(".")
from molgri.io import OrcaWriter, OrcaReader, QuantumMolecule, QuantumSetup, read_important_stuff_into_csv
from molgri.space.fullgrid import FullGrid
from molgri.paths import PATH_INPUT_BASEGRO, PATH_EXPERIMENTS


configfile: "workflow/default_sqra_orca_config.yaml"

NUM_GRID_POINTS = len(FullGrid(
    b_grid_name=str(config["params_grid"]["num_orientations"]),
    o_grid_name=str(config["params_grid"]["num_directions"]),
    t_grid_name=config["params_grid"]["radial_distances_nm"]))

def _determine_batch_subfolders():
    all_paths = []
    for batch in range(config['num_batches']):
        section_size = NUM_GRID_POINTS//config['num_batches'] +1
        batch_start_index = batch*section_size
        batch_end_index = np.min([(batch+1)*section_size, NUM_GRID_POINTS])
        all_paths.extend([f"batch_{batch}/{str(i).zfill(10)}/" for i in range(batch_start_index, batch_end_index)])
    return all_paths

def _determine_batch_folders(wildcards, file_needed):
    all_paths = []
    for batch in range(config['num_batches']):
        section_size = NUM_GRID_POINTS//config['num_batches'] +1
        batch_start_index = batch*section_size
        batch_end_index = np.min([(batch+1)*section_size, NUM_GRID_POINTS])
        all_paths.extend([f"{wildcards.where}batch_{batch}/{str(i).zfill(10)}/{file_needed}" for i in range(batch_start_index, batch_end_index)])
    return all_paths


def determine_output_files_in_batches(wildcards):
    return _determine_batch_folders(wildcards, "orca.out")

def determine_input_files_in_batches(wildcards):
    return _determine_batch_folders(wildcards,"orca.inp")

rule modify_run_orca:
    input:
        script_run_orca_job= "molgri/scripts/run_ORCA.sh",
    output:
        script_run_orca_job="{where}run_ORCA.sh",
    run:
        with open(input.script_run_orca_job, "r") as f:
            all_lines = f.readlines()

        for i, line in enumerate(all_lines):
            if line.startswith("#SBATCH --time="):
                all_lines[i] = f"#SBATCH --time={config['params_dft']['max_runtime']}\n"
            if line.startswith("#SBATCH --mem-per-cpu="):
                all_lines[i] = f"#SBATCH --mem-per-cpu={config['params_dft']['max_mem_per_cpu']}\n"

        with open(output.script_run_orca_job,"w") as f:
            f.writelines(all_lines)


rule copy_to_curta:
    input:
        ready_to_copy = f"{{where}}all_inputs_exist.touch",
        script_run_orca_job = "{where}run_ORCA.sh",
        script_submit_all_jobs = "molgri/scripts/submit_on_curta.sh",
    output:
        copied=touch(f"{{where}}all_inputs_copied.touch"),
    run:
        calculation_directory = wildcards.where
        # make directories that may be missing
        subprocess.run(f"""ssh hanaz63@curta.zedat.fu-berlin.de 'mkdir -p /home/hanaz63/{calculation_directory}'""",shell=True)
        # copy script_run_orca_job
        subprocess.run(f"""rsync -ruPE {input.script_run_orca_job} hanaz63@curta.zedat.fu-berlin.de:/home/hanaz63/{calculation_directory}/run_ORCA.sh""",shell=True)
        # copy submission loop file
        subprocess.run(f"""rsync -ruPE {input.script_submit_all_jobs} hanaz63@curta.zedat.fu-berlin.de:/home/hanaz63/{calculation_directory}/submit_on_curta.sh""",shell=True)
        # copy all inp files
        subprocess.run(f"""rsync -ruaPE --compress --exclude='*tex' --exclude='*cpcm*' --exclude='*densities*' --exclude='*engrad' --exclude='*.gbw' --exclude='*.txt' --exclude='*smd*' --exclude='slurm*' --exclude='*/directory' --exclude='*tmp*'  --include='*.inp' --exclude='*.xyz' {calculation_directory} hanaz63@curta.zedat.fu-berlin.de:/home/hanaz63/{calculation_directory}""",shell=True)

rule run_on_curta:
    input:
        copied=f"{{where}}all_inputs_copied.touch"
    output:
        ran=touch(f"{{where}}all_inputs_ran.touch")
    run:
        # calculate max size of the batch
        section_size = NUM_GRID_POINTS // config['num_batches'] + 1
        assert section_size < 5000, "Curta can only handle up to 5000 jobs in an array!"
        subprocess.run(f"""ssh hanaz63@curta.zedat.fu-berlin.de 'cd /home/hanaz63/{wildcards.where} && ./submit_on_curta.sh {section_size}'""",shell=True)

rule get_from_curta:
    input:
        ran = f"{{where}}all_inputs_ran.touch"
    output:
        copied=touch(f"{{where}}all_outputs_copied.touch"),
    run:
        calculation_directory = wildcards.where
        subprocess.run(f"""rsync -ruaPE  --compress --exclude='*.sh' --exclude='*.opt' --exclude='*ges' --exclude='*tex' --exclude='*cpcm*' --exclude='*densities*' --exclude='*engrad' --exclude='*.gbw' --exclude='*.inp' --exclude='*.txt' --exclude='*smd*' --exclude='slurm*' --exclude='*/directory' --exclude='*tmp*' hanaz63@curta.zedat.fu-berlin.de:/home/hanaz63/{calculation_directory} {calculation_directory}""",shell=True)


rule split_inpfile:
    """
    If needed (e.g. for orca calculations) split a single file trajectory info a folder of single-structure files named 
    from 0000000000 to the total num of points.

    Warning, this is not a general tool for any .xyz files but specifically for my pseudotrajectory file.
    """
    input:
        full_inp=f"{{where}}full_orca.inp",
    output:
        all_out = expand(f"{{where}}{{specific_paths}}orca.inp",
            specific_paths=_determine_batch_subfolders(), allow_missing=True),
        to_touch = touch(f"{{where}}all_inputs_exist.touch")
    run:
        with open(input.full_inp, "r") as f:
            print(input.full_inp)
            all_lines = f.readlines() # throwing away the last \n line

        split_len = len(all_lines)//NUM_GRID_POINTS

        for i, output_file in enumerate(output.all_out):
            with open(output_file, "w") as f:
                f.writelines(all_lines[i*split_len:(i+1)*split_len])


rule make_orca_inp_whole_traj:
    input:
        xyz_file = f"{{where}}trajectory.xyz",
        num_atoms= f"{{where}}num_atoms.txt"
    output:
        inp_file = f"{{where}}full_orca.inp",
    params:
        charge = config["params_dft"]["charge"],
        multiplicity = config["params_dft"]["multiplicity"],
    run:
        with open(input.num_atoms) as f:
            first_fragment = int(f.readline())
            second_fragment = int(f.readline())

        dimer = QuantumMolecule(charge=params.charge,multiplicity=params.multiplicity,xyz_file=input.xyz_file,
                                fragment_1_len=first_fragment, fragment_2_len=second_fragment)

        ob = OrcaWriter(dimer,QUANTUM_SETUP)
        geo_optimization = config["params_dft"]["optimization_type"] == "Opt"
        constrain_fragments = config["params_dft"]["optimization_type"] == "ConstOpt"
        ob.make_entire_trajectory_inp(geo_optimization=geo_optimization, constrain_fragments=constrain_fragments)
        ob.write_to_file(output.inp_file)

rule orca_run_multi_inp:
    input:
        inp_file = "{where}full_orca.inp",
        script_run_orca_job = "{where}run_ORCA.sh"
    output:
        copied = touch(f"{{where}}all_inputs_copied.touch")
    run:
        subprocess.run(f"""scp {input.inp_file}""")



rule orca_check_output:
    """
    For every PT structure run a corresponding orca calculation (SP/full optimization/constrained optimization)
    """
    input:
        inp_file = f"{{where}}all_outputs_copied.touch"
    output:
        out_file = touch(expand(f"{{where}}{{batch_subf}}orca.out", batch_subf=_determine_batch_subfolders(), allow_missing=True))


rule get_optimized_pt:
    """
    The opposite to split xyz file.
    """
    input:
        determine_output_files_in_batches
    output:
        trajectory=f"{{where}}trajectory_opt.xyz",
    run:
        from molgri.io import OrcaReader
        all_xyz = ""
        for out_file in input:
            # if calculation was successful, add the last coordinates
            my_reader = OrcaReader(out_file)
            current_xyz = my_reader.extract_optimized_xyz()
            all_xyz += current_xyz
        with open(output.trajectory, "w") as f:
            f.write(all_xyz)



rule orca_collect_energies:
    """
    After energies for each point in trajectory have been calculated, combine them for
    """
    input:
        determine_output_files_in_batches
    output:
        energy = f"{{where}}energy.csv"
    run:
        # note: since differences in energy are used, there is no need for ZPE-corrected energy
        # TODO: make sure to allow for control of the number of cycles and deal with failing structures
        read_important_stuff_into_csv(out_files_to_read=input, csv_file_to_write=output.energy,
            setup=QUANTUM_SETUP)



rule orca_total_and_average_time:
    input:
        energy = f"{{where}}energy.csv"
    output:
        total_time = f"{{where}}orca_time.txt"
    run:
        import pandas as pd

        my_df = pd.read_csv(input.energy)

        time_s = my_df["Time [s]"]

        with open(output.total_time, "w") as f:
            f.write(f"Total time [s]: {time_s.sum():.2f}\n")
            f.write(f"Mean time [s]: {time_s.mean():.2f} +- {time_s.std():.2f}\n")
            f.write(f"Max time [s]: {time_s.max():.2f}\n")
            f.write(f"Min time [s]: {time_s.min():.2f}\n")
            f.write("--------------\n")
            f.write(f"Total time [h:m:s]: {pd.to_timedelta(time_s.sum(), unit='s')}\n")
            f.write(f"Mean time [h:m:s]: {pd.to_timedelta(time_s.mean(), unit='s')} +- {pd.to_timedelta(time_s.std(), unit='s')}\n")
            f.write(f"Max time [h:m:s]: {pd.to_timedelta(time_s.max(), unit='s')}\n")
            f.write(f"Min time [h:m:s]: {pd.to_timedelta(time_s.min(), unit='s')}\n")
