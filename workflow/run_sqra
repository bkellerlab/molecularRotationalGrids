"""
All of the workflows relating to pseudotrajectories, subsequent SQRAs and their related outputs (figures ...)
"""

import sys
import numpy as np

import matplotlib.pyplot as plt
plt.switch_backend('agg')

# add molgri directory
sys.path.append(".")
from workflow.snakemake_utils import modify_mdrun, modify_topology
from molgri.paths import PATH_INPUT_BASEGRO, PATH_EXPERIMENTS, PATH_EXAMPLES

include: "run_grid"

if config["experiment_type"] not in ["sqra_water_in_vacuum", "guanidinium_xyz", "sqra_fullerene", "sqra_bpti_trypsine"]:
    raise AttributeError(f"This pipeline should be used for SQRA experiments in vacuum. Your experiment type is {config['experiment_type']} instead of 'sqra_water_in_vacuum'")

EXPERIMENT_TYPE = config["experiment_type"]
EXPERIMENT_ID = config["experiment_id"]
GRID_ID = config["grid_identifier"]
STRUCTURE_EXTENSION = config["structure_extension"]

try:
    NUM_REPEATS = config["num_repeats"]
except KeyError:
    # not a benchmarking run, normal calculation run
    NUM_REPEATS = 1

rule all_sqra:
    input:
        expand(f"{PATH_EXPERIMENTS}{EXPERIMENT_TYPE}/{EXPERIMENT_ID}/{GRID_ID}/{{what}}",
            what=["its.png", "eigenvectors_vmdlog", "eigenvalues.png", "its.csv"]),
    #what=["its.png", "eigenvectors_vmdlog", "eigenvalues.png", "its.csv"]
    log:
        log = f"{PATH_EXPERIMENTS}{EXPERIMENT_TYPE}/{EXPERIMENT_ID}/{GRID_ID}/total_log.yaml"
    run:
        import yaml
        with open(log.log, "w")  as f:
            yaml.dump(config, f)

rule copy_structures:
    """
    Copying m1 and m2 must be a separate rule because they may have extensions .xyz, .gro, .pdb ...
    """
    input:
        m1 = f"{PATH_INPUT_BASEGRO}{EXPERIMENT_TYPE}/m1.{STRUCTURE_EXTENSION}",
        m2 = f"{PATH_INPUT_BASEGRO}{EXPERIMENT_TYPE}/m2.{STRUCTURE_EXTENSION}",
    output:
        m1 = f"{PATH_EXPERIMENTS}{EXPERIMENT_TYPE}/{EXPERIMENT_ID}/{GRID_ID}/m1.{STRUCTURE_EXTENSION}",
        m2 = f"{PATH_EXPERIMENTS}{EXPERIMENT_TYPE}/{EXPERIMENT_ID}/{GRID_ID}/m2.{STRUCTURE_EXTENSION}",
    run:
        import shutil
        shutil.copy(input.m1, output.m1)
        shutil.copy(input.m2, output.m2)

rule find_num_atoms:
    """
    Copying m1 and m2 must be a separate rule because they may have extensions .xyz, .gro, .pdb ...
    """
    input:
        m1 = f"{PATH_INPUT_BASEGRO}{EXPERIMENT_TYPE}/m1.{STRUCTURE_EXTENSION}",
        m2 = f"{PATH_INPUT_BASEGRO}{EXPERIMENT_TYPE}/m2.{STRUCTURE_EXTENSION}",
    output:
        num_atoms = f"{PATH_EXPERIMENTS}{EXPERIMENT_TYPE}/{EXPERIMENT_ID}/{GRID_ID}/num_atoms.txt"
    run:
        from molgri.io import OneMoleculeReader
        N_m1 = len(OneMoleculeReader(input.m1).get_molecule().atoms)
        N_m2 = len(OneMoleculeReader(input.m2).get_molecule().atoms)

        with open(output.num_atoms, "w") as f:
            f.write(f"{N_m1}\n")
            f.write(f"{N_m2}\n")

rule copy_input:
    """
    Copy the rest of necessary files to start a sqra run and potentially adapt default mdrun params.
    """
    input:
        dimer_topology = f"{PATH_INPUT_BASEGRO}{EXPERIMENT_TYPE}/topol.top",
        select_energy = f"{PATH_INPUT_BASEGRO}{EXPERIMENT_TYPE}/select_energy_five",
        runfile= f"{PATH_INPUT_BASEGRO}{EXPERIMENT_TYPE}/mdrun.mdp",
        force_field_stuff = f"{PATH_INPUT_BASEGRO}{EXPERIMENT_TYPE}/force_field_stuff/"
    output:
        dimer_topology = f"{PATH_EXPERIMENTS}{EXPERIMENT_TYPE}/{EXPERIMENT_ID}/{GRID_ID}/topol.top",
        select_energy = f"{PATH_EXPERIMENTS}{EXPERIMENT_TYPE}/{EXPERIMENT_ID}/{GRID_ID}/select_energy",
        runfile = f"{PATH_EXPERIMENTS}{EXPERIMENT_TYPE}/{EXPERIMENT_ID}/{GRID_ID}/mdrun.mdp",
        force_field_stuff = directory(f"{PATH_EXPERIMENTS}{EXPERIMENT_TYPE}/{EXPERIMENT_ID}/{GRID_ID}/force_field_stuff/")
    params:
        tau_t = config["params_setup"]["tau_t"],
        integrator = config["params_setup"]["integrator"],
        energy_writeout_frequency= config["params_setup"]["energy_writeout_frequency"],
        cutoff_scheme = config["params_setup"]["cutoff-scheme"],
        epsilon = config["params_setup"]["epsilon"]
    run:
        import shutil
        shutil.copy(input.select_energy,output.select_energy)
        shutil.copy(input.dimer_topology, output.dimer_topology)
        shutil.copy(input.runfile,output.runfile)
        shutil.copytree(input.force_field_stuff,output.force_field_stuff, dirs_exist_ok=True)

        # modifying files
        modify_mdrun(output.runfile, "tau_t", params.tau_t)
        modify_mdrun(output.runfile,"integrator",params.integrator)
        modify_mdrun(output.runfile,"nstenergy",params.energy_writeout_frequency)
        modify_mdrun(output.runfile, "pcoupl", "no")
        modify_mdrun(output.runfile,"coulombtype",params.cutoff_scheme)
        modify_mdrun(output.runfile, "epsilon-r", params.epsilon)

# rule set_up_guanidinium_xyz:
#     """
#     Copy and prepare all the files to be able to run simulations or grid calculations for water in vacuum.
#     """
#     wildcard_constraints:
#         experiment_type=".*guanidinium.*"
#     input:
#         structure = f"{PATH_INPUT_BASEGRO}Guanidinium/guanidinium.xyz",
#     output:
#         molecule1 = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/m1.xyz",
#         molecule2 = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/m2.xyz",
#     params:
#         tau_t = config["params_setup"]["tau_t"],
#         up1_nm = config["params_setup"]["up1_nm"],
#         up2_nm = config["params_setup"]["up2_nm"],
#         force_constant_restraint = config["params_setup"]["force_constant_restraint"],
#     run:
#         # first copy all inputs to outputs
#         import shutil
#         shutil.copy(input.structure,output.molecule1)
#         shutil.copy(input.structure,output.molecule2)


# rule run_pt_guanidinium:
#     """
#     This rule should produce the .gro and .xtc files of the pseudotrajectory.
#     """
#     wildcard_constraints:
#         experiment_type=".*guanidinium.*"
#     input:
#         molecule1 = rules.set_up_guanidinium_xyz.output.molecule1,
#         molecule2 = rules.set_up_guanidinium_xyz.output.molecule2,
#         grid = rules.run_grid.output.full_array
#     output:
#         structure = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/structure.xyz",
#         trajectory = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/trajectory.xyz"
#     params:
#         cell_size_A = config["params_sqra"]["cell_size_A"]  # cubic box will be the output, this is size of the box in one dimension
#     run:
#         from molgri.io import PtWriter
#         my_writer = PtWriter(input.molecule1, input.molecule2, cell_size_A=params.cell_size_A, path_grid=input.grid)
#         my_writer.write_full_pt(path_output_pt=output.trajectory, path_output_structure=output.structure)

rule run_pt:
    """
    This rule should produce the .gro and .xtc files of the pseudotrajectory.
    """
    input:
        molecule1 = rules.copy_structures.output.m1,
        molecule2 = rules.copy_structures.output.m2,
        grid = rules.run_grid.output.full_array
    output:
        structure = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/structure.gro",
        trajectory = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/trajectory.xtc"
    params:
        cell_size_A = config["params_sqra"]["cell_size_A"]  # cubic box will be the output, this is size of the box in one dimension
    benchmark:
        repeat(f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/pt_benchmark.txt", NUM_REPEATS)
    run:
        from molgri.io import PtWriter
        my_writer = PtWriter(input.molecule1, input.molecule2, cell_size_A=params.cell_size_A, path_grid=input.grid)
        my_writer.write_full_pt(path_output_pt=output.trajectory, path_output_structure=output.structure)


rule gromacs_rerun:
    """
    This rule gets structure, trajectory, topology and gromacs run file as input, as output we are only interested in
    energies.
    """

    input:
        structure = rules.run_pt.output.structure,
        trajectory = rules.run_pt.output.trajectory,
        runfile = rules.copy_input.output.runfile,
        topology = rules.copy_input.output.dimer_topology,
        select_energy = rules.copy_input.output.select_energy,
        force_field_stuff = rules.copy_input.output.force_field_stuff
    shadow: "minimal"
    log:
        log = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/logging_gromacs.log"
    benchmark:
        repeat(f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/gromacs_benchmark.txt", NUM_REPEATS)
    output:
        energy = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/energy.xvg",
    # use with arguments like path_structure path_trajectory path_topology path_default_files path_output_energy
    params:
        maxwarn = config["params_sqra"]["maxwarn"]
    shell:
        """
        initial_dir=$(pwd)
        cd $(dirname {input.structure})
        export PATH="/home/janjoswig/local/gromacs-2022/bin:$PATH"
        gmx22 grompp -f $(basename {input.runfile}) -c $(basename {input.structure}) -p $(basename {input.topology}) -o result.tpr  -maxwarn {params.maxwarn}
        gmx22 mdrun -s result.tpr -rerun $(basename {input.trajectory}) -g $(basename {log.log})
        gmx22 energy -f ener.edr -o $(basename {output.energy}) < $(basename {input.select_energy})
        cd "$initial_dir" || exit
        """


rule lowest_e_structures:
    input:
        energy = rules.gromacs_rerun.output.energy
    output:
        list_structures = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/indices_lowest_E.csv"
    params:
        num = config["params_sqra"]["number_lowest_E_structures"]
    run:
        from molgri.space.utils import k_argmin_in_array
        from molgri.io import EnergyReader
        er = EnergyReader(input.energy).load_energy()
        energies2 = er["Coulomb (SR)"]
        energies5 = er["LJ (SR)"]
        energies6 = er["Potential"]
        energies7 = er["Disper. corr."]

        all_lowest_ind = k_argmin_in_array(energies6, k=params.num)
        all_lowest_E = energies6[all_lowest_ind]
        sort_index = np.argsort(all_lowest_E)
        sorted_indices = all_lowest_ind[sort_index]
        print("POTENTIAL")
        print(", ".join([str(x+1) for x in all_lowest_ind]))
        print(all_lowest_E)
        np.savetxt(output.list_structures, sorted_indices)


rule run_sqra:
    """
    As input we need: energies, adjacency, volume, borders, distances.
    As output we want to have the rate matrix.
    """
    input:
        energy = rules.gromacs_rerun.output.energy,
        distances_array = rules.run_grid.output.distances_array,
        borders_array = rules.run_grid.output.borders_array,
        volumes = rules.run_grid.output.volumes,
    output:
        rate_matrix = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/rate_matrix.npz",
        index_list = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/index_list.npy",
    benchmark:
        repeat(f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/rate_matrix_benchmark.txt", NUM_REPEATS)
    params:
        T=float(config["params_sqra"]["temperature_K"]),
        energy_type=config["params_sqra"]["energy_type"],
        m_h2o = float(config["params_sqra"]["mass_kg"]),
        tau = float(config["params_setup"]["tau_t"]),
        lower_lim = config["params_sqra"]["lower_lim_rate_matrix"],
        upper_lim = config["params_sqra"]["upper_lim_rate_matrix"],
    run:
        from molgri.io import EnergyReader
        from molgri.molecules.transitions import SQRA
        from scipy import sparse
        from scipy.constants import k as k_B
        import pandas as pd



        tau = float(params.tau) * 1e-12 # now in s

        D = k_B * params.T *tau / params.m_h2o  # in m^2/s
        D*= 1e8  # now in A^2/ps
        print(f"Diffusion const D={D} ")

        # load input files
        all_volumes = np.load(input.volumes)
        all_surfaces = sparse.load_npz(input.borders_array)
        all_distances = sparse.load_npz(input.distances_array)

        # determine limits
        if params.lower_lim == "None":
            lower_limit = None
        else:
            lower_limit = float(params.lower_lim)
        if params.upper_lim == "None":
            upper_limit = None
        else:
            upper_limit = float(params.upper_lim)


        energies = EnergyReader(input.energy).load_single_energy_column(params.energy_type)

        sqra = SQRA(energies=energies,volumes=all_volumes,distances=all_distances,surfaces=all_surfaces)



        rate_matrix = sqra.get_rate_matrix(D,params.T)
        print(np.max(rate_matrix.data), np.min(rate_matrix.data), np.max(-rate_matrix.data), np.min(-rate_matrix.data))
        rate_matrix, index_list = sqra.cut_and_merge(rate_matrix,T=params.T,lower_limit=lower_limit,
            upper_limit=upper_limit)
        # saving to file
        sparse.save_npz(output.rate_matrix,rate_matrix)
        np.save(output.index_list,np.array(index_list,dtype=object))


rule run_clustering:
    input:
        eigenvectors = f"experiments/SQRA_electrostatic_1_cc_0001/80_80_very_short/None-None/eigenvectors_0_SR.npy",
        #eigenvectors= f"/home/hanaz63/2024_molgri2/nobackup/important_experiments_backup/vacuum_msm_02/80_80_very_short/10/eigenvectors_msm_None_LR.npy",
        assignments = f"/home/hanaz63/2024_molgri2/nobackup/important_experiments_backup/vacuum_msm_02/80_80_very_short/assignments.npy",
        #rate_matrix= f"{PATH_EXPERIMENTS}one_sqra/small_ideal/None-None/rate_matrix.npz",
    params:
        tau_msm = 0.1,
        num_states = 4
    run:
        from scipy import sparse
        from sklearn_extra.cluster import CommonNNClustering
        from sklearn.cluster import DBSCAN, KMeans, OPTICS
        import pandas as pd

        eigenvectors = np.load(input.eigenvectors)[:,:6]
        print("SHAPE", eigenvectors.shape)

        # OPTICS min samples 10 max eps inf

        # for eps in [0.0000001]:
        #     for min_samples in [2 ]:
        #         #clustering = CommonNNClustering(eps=eps, min_samples=min_samples).fit(eigenvectors)
        #         #print("EPS", eps, "MIN", min_samples)
        #         clustering = KMeans(n_clusters=12).fit(eigenvectors)
        #         my_labels = clustering.labels_
        #         unique, counts = np.unique(my_labels, return_counts=True)
        #         print(unique[np.where(counts>10)[0]], counts[np.where(counts>10)[0]])
        #         np.save("labels_msm.npy", my_labels)


        import matplotlib.pyplot as plt
        import seaborn as sns

        sns.set_style("white")
        fig, ax = plt.subplots(1,1,subplot_kw={"projection": "3d"})
        c = np.load("labels.npy").astype(int)
        palette_sqra = ["black", "yellow", "orange", "green", "blue", "cyan", "purple", "gray", "pink", "red"]  #pop over 10
        #palette = ["black", "yellow", "orange", "green", "blue", "cyan", "purple", "gray", "pink", "red"]   # pop over 20

        assignments = np.load(input.assignments)
        first_evec = eigenvectors.T[1]
        second_evec = eigenvectors.T[2]
        third_evec = eigenvectors.T[3]
        unique, counts = np.unique(c,return_counts=True)
        #print(unique, counts)
        for i, label in enumerate(unique[np.where(counts>1)[0]]):
            cluster = np.where(c == label)[0]
            #ssign = np.nonzero(np.in1d(assignments,cluster))[0]
            #assign = np.where([assignments==k for k in cluster])[0]
            #print(assign[:10], assign.shape)
            #print([pa[i],]*len(cluster))
            ax.scatter(first_evec[cluster],second_evec[cluster],third_evec[cluster],c=[palette_sqra[i],]*len(cluster))
            population = len(cluster)
            if population > 50:
                print(f"{label} with population {len(cluster)} ######## \n",", ".join([str(x + 1) for x in np.random.choice(cluster,30)]))
            else:
                print(f"{label} with population {len(cluster)} ######## \n",", ".join([str(x + 1) for x in cluster]))
            print()

        plt.savefig("myplot.png", dpi=600)
        #plt.show()

        # for label, count in zip(unique, counts):
        #     if count > 100:




        # non-negative elements
        # assert np.all(transition_matrix >= 0), "Contains negative elements"
        # # elements of each row sum up to one


        #.
        #
        # for i, row in enumerate(sym_transition_matrix):
        #     sym_transition_matrix[i] /= np.sum(row)
        #     #if not np.isclose(np.sum(row), 1):
        #     #    print(np.sum(row))
        #
        # msm = MarkovStateModel(sym_transition_matrix,stationary_distribution=stationary_distribution,
        #     reversible=True,n_eigenvalues=None,ncv=None,count_model=None,transition_matrix_tolerance=1e-04,lagtime=None)
        # print(msm.transition_matrix_tolerance)
        #
        # my_pcca = msm.pcca(params.num_states)
        #
        # print(my_pcca.__dict__)

rule run_decomposition:
    """
    As output we want to have eigenvalues, eigenvectors. Es input we get a (sparse) rate matrix.
    """
    input:
        rate_matrix = rules.run_sqra.output.rate_matrix
    output:
        eigenvalues = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/eigenvalues.npy",
        eigenvectors = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/eigenvectors.npy",
    benchmark:
        repeat(f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/decomposition_benchmark.txt", NUM_REPEATS)
    params:
        tol=config["params_sqra"]["tol"],
        maxiter=config["params_sqra"]["maxiter"],
        sigma=config["params_sqra"]["sigma"],
        which=config["params_sqra"]["which"],
    run:
        from scipy import sparse
        from molgri.molecules.transitions import DecompositionTool

        # loading
        my_matrix = sparse.load_npz(input.rate_matrix)

        if params.sigma == "None":
            sigma = None
        else:
            sigma = float(params.sigma)
        # calculation
        dt = DecompositionTool(my_matrix)
        all_eigenval, all_eigenvec = dt.get_decomposition(tol=params.tol, maxiter=params.maxiter, which=params.which,
            sigma=sigma)

        # saving to file
        np.save(output.eigenvalues,np.array(all_eigenval))
        np.save(output.eigenvectors,np.array(all_eigenvec))


rule run_plot_everything_sqra:
    """
    Make a plot of eigenvalues
    """
    input:
        eigenvalues = rules.run_decomposition.output.eigenvalues,
        eigenvectors = rules.run_decomposition.output.eigenvectors,
    output:
        plot_eigenvectors=f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/eigenvectors.png",
        plot_eigenvalues=f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/eigenvalues.png",
        plot_its=report(f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/its.png", category="{experiment_id}"),
    run:
        from molgri.plotting.transition_plots import PlotlyTransitions

        pt = PlotlyTransitions(is_msm=False,path_eigenvalues=input.eigenvalues,path_eigenvectors=input.eigenvectors)
        # eigenvectors
        pt.plot_eigenvectors_flat()
        pt.save_to(output.plot_eigenvectors,height=800, width=400)
        # eigenvalues
        pt.plot_eigenvalues()
        pt.save_to(output.plot_eigenvalues)
        # # its for msm
        pt.plot_its_as_line()
        pt.save_to(output.plot_its)
        # we could also plot the heatmap of the matrix, but it's honestly not that useful and can become very large

rule compile_vmd_log:
    """
    Input are the saved eigenvectors. Output = a vmd log that can be used later with:

    vmd <gro file> <xtc file>
    play <vmdlog file>
    """
    input:
        structure = rules.run_pt.output.structure,
        trajectory = rules.run_pt.output.trajectory,
        eigenvectors = rules.run_decomposition.output.eigenvectors,
        index_list = rules.run_sqra.output.index_list,
        num_atoms = rules.find_num_atoms.output.num_atoms
    output:
        vmdlog = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/eigenvectors_vmdlog",
        fig_tga = expand(f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/eigenvector{{i}}.tga", i=[0, 1, 2, 3, 4, 5], allow_missing=True),
        fig_png= report(expand(f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/eigenvector{{i}}.png",i=[0, 1, 2, 3, 4, 5],allow_missing=True),
        category="{experiment_type}")
    params:
        num_extremes=config["params_sqra"]["num_extremes_to_plot"],
        num_eigenvec=config["params_sqra"]["num_eigenvec_to_plot"]
    run:
        from molgri.plotting.create_vmdlog import VMDCreator

        # load eigenvectors
        eigenvectors = np.load(input.eigenvectors)

        # determine first and second index
        with open(input.num_atoms, "r") as f:
            num_first_mol = int(f.readline().strip())

        index_first = f"index < {num_first_mol}"
        index_second = f"index >= {num_first_mol}"

        if wildcards.experiment_type == "sqra_bpti_trypsine":
            is_protein=True
        else:
            is_protein=False

        vmd_creator = VMDCreator(EXPERIMENT_TYPE, index_first_molecule=index_first, index_second_molecule=index_second, is_protein=is_protein)


        index_list = np.load(input.index_list,allow_pickle=True)
        if not np.any(index_list):
            index_list = None
        else:
            index_list = list(index_list)

        vmdlog = vmd_creator.prepare_eigenvector_script(eigenvectors.T, plot_names=output.fig_tga, index_list=index_list,
            n_eigenvectors=params.num_eigenvec, num_extremes=params.num_extremes)

        with open(output.vmdlog, "w") as f:
            f.write(vmdlog)

        shell("vmd  -dispdev text {input.structure} {input.trajectory} < {output.vmdlog}")
        for el_tga, el_png in zip(output.fig_tga, output.fig_png):
            shell("convert {el_tga} {el_png}")

rule print_its:
    input:
        eigenvalues = rules.run_decomposition.output.eigenvalues
    output:
        data = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/its.csv"
    run:
        import pandas as pd

        all_its = []
        eigenvals = np.load(input.eigenvalues)[1:]  # dropping the first one as it should be zero and cause issues
        all_its.append([-1 / (eigenval) for eigenval in eigenvals])
        my_df = pd.DataFrame(all_its, columns=[f"ITS {i} [ps]" for i in range(1, len(all_its[0])+1)])
        my_df.to_csv(output.data)



