"""
All the workflows relating to pseudotrajectories, subsequent SQRAs and their related outputs (figures ...)
"""
import os
import sys
import numpy as np
import subprocess

import matplotlib.pyplot as plt
plt.switch_backend('agg')

# add molgri directory
sys.path.append(".")
from workflow.snakemake_utils import modify_mdrun, modify_topology
from molgri.paths import PATH_INPUT_BASEGRO, PATH_EXPERIMENTS, PATH_EXAMPLES
from molgri.molecules.orca_runner import read_important_stuff_into_csv, QuantumMolecule, QuantumSetup, make_inp_file, assert_normal_finish
from molgri.space.fullgrid import FullGrid

include: "run_grid"
#include: "run_orca"

if config["experiment_type"] not in ["sqra_water_in_vacuum", "guanidinium_xyz", "sqra_fullerene", "sqra_bpti_trypsine", "water_xyz", "fullerene_xyz"]:
    raise AttributeError(f"This pipeline should be used for SQRA experiments in vacuum. Your experiment type is {config['experiment_type']} instead of 'sqra_water_in_vacuum'")

EXPERIMENT_TYPE = config["experiment_type"]
EXPERIMENT_ID = config["experiment_id"]
GRID_ID = config["grid_identifier"]
STRUCTURE_EXTENSION = config["structure_extension"]
ENERGY_PROGRAM = config["energy_program"]
NUM_GRID_POINTS = len(FullGrid(
    b_grid_name=str(config["params_grid"]["num_orientations"]),
    o_grid_name=str(config["params_grid"]["num_directions"]),
    t_grid_name=config["params_grid"]["radial_distances_nm"]))


PROVIDED_DATA_PATH =  f"{PATH_INPUT_BASEGRO}{EXPERIMENT_TYPE}/"
EXPERIMENT_FULL_PATH = f"{PATH_EXPERIMENTS}{EXPERIMENT_TYPE}/{EXPERIMENT_ID}/{GRID_ID}/"

if ENERGY_PROGRAM == "ORCA":
    QUANTUM_SETUP = QuantumSetup(
        functional=config["params_dft"]["functional"],
        basis_set=config["params_dft"]["basis_set"],
        solvent=config["params_dft"]["solvent"],
        dispersion_correction=config["params_dft"]["dispersion"],
        num_scf=config["params_dft"]["num_scf"],
        num_cores=config["params_dft"]["num_cores"],
        ram_per_core = config["params_dft"]["ram_per_core"]
    )
    ORCA_DIR = QUANTUM_SETUP.get_dir_name()
    TRAJECTORY_EXTENSION = "xyz"
else:
    QUANTUM_SETUP = None
    ORCA_DIR = None
    TRAJECTORY_EXTENSION = "xtc"
try:
    NUM_REPEATS = config["num_repeats"]
except KeyError:
    # not a benchmarking run, normal calculation run
    NUM_REPEATS = 1

rule all_sqra:
    input:
        #f"{EXPERIMENT_FULL_PATH}energy.csv"
        expand(f"{EXPERIMENT_FULL_PATH}{{what}}", what=["energy.csv"])
    #what=["its.png", "eigenvectors_vmdlog", "eigenvalues.png", "its.csv", "energy.csv", "orca_time.txt"]
    log:
        log = f"{EXPERIMENT_FULL_PATH}total_log.yaml"
    benchmark:
        repeat(f"{EXPERIMENT_FULL_PATH}total_benchmark.txt", NUM_REPEATS)
    run:
        import yaml
        with open(log.log, "w")  as f:
            yaml.dump(config, f)

rule copy_structures:
    """
    Copying m1 and m2 is a separate rule because they may have extensions .xyz, .gro, .pdb ...
    """
    input:
        m1 = f"{PROVIDED_DATA_PATH}m1.{STRUCTURE_EXTENSION}",
        m2 = f"{PROVIDED_DATA_PATH}m2.{STRUCTURE_EXTENSION}",
    output:
        m1 = f"{EXPERIMENT_FULL_PATH}m1.{STRUCTURE_EXTENSION}",
        m2 = f"{EXPERIMENT_FULL_PATH}m2.{STRUCTURE_EXTENSION}",
    run:
        import shutil
        shutil.copy(input.m1, output.m1)
        shutil.copy(input.m2, output.m2)

rule find_num_atoms:
    """
    Create a simple file where the first line is the number of atoms in molecule 1 and the second line the number of 
    atoms in molecule 2.
    """
    input:
        m1 = rules.copy_structures.output.m1,
        m2 = rules.copy_structures.output.m2,
    output:
        num_atoms = f"{EXPERIMENT_FULL_PATH}num_atoms.txt"
    run:
        from molgri.io import OneMoleculeReader
        N_m1 = len(OneMoleculeReader(input.m1).get_molecule().atoms)
        N_m2 = len(OneMoleculeReader(input.m2).get_molecule().atoms)

        with open(output.num_atoms, "w") as f:
            f.write(f"{N_m1}\n")
            f.write(f"{N_m2}\n")

rule copy_gromacs_input:
    """
    Copy the rest of necessary files to start a sqra run with a GROMACS calculation and potentially adapt default mdrun params.
    """
    wildcard_constraints:
        EXPERIMENT_ID = "GROMACS"
    input:
        dimer_topology = f"{PROVIDED_DATA_PATH}topol.top",
        select_energy = f"{PROVIDED_DATA_PATH}select_energy_five",
        runfile= f"{PROVIDED_DATA_PATH}mdrun.mdp",
        force_field_stuff = f"{PROVIDED_DATA_PATH}force_field_stuff/"
    output:
        dimer_topology = f"{EXPERIMENT_FULL_PATH}topol.top",
        select_energy = f"{EXPERIMENT_FULL_PATH}select_energy",
        runfile = f"{EXPERIMENT_FULL_PATH}mdrun.mdp",
        force_field_stuff = directory(f"{EXPERIMENT_FULL_PATH}force_field_stuff/")
    params:
        tau_t = config["params_setup"]["tau_t"],
        integrator = config["params_setup"]["integrator"],
        energy_writeout_frequency= config["params_setup"]["energy_writeout_frequency"],
        cutoff_scheme = config["params_setup"]["cutoff-scheme"],
        epsilon = config["params_setup"]["epsilon"]
    run:
        import shutil
        shutil.copy(input.select_energy,output.select_energy)
        shutil.copy(input.dimer_topology, output.dimer_topology)
        shutil.copy(input.runfile,output.runfile)
        shutil.copytree(input.force_field_stuff,output.force_field_stuff, dirs_exist_ok=True)

        # modifying files
        modify_mdrun(output.runfile, "tau_t", params.tau_t)
        modify_mdrun(output.runfile,"integrator",params.integrator)
        modify_mdrun(output.runfile,"nstenergy",params.energy_writeout_frequency)
        modify_mdrun(output.runfile, "pcoupl", "no")
        modify_mdrun(output.runfile,"coulombtype",params.cutoff_scheme)
        modify_mdrun(output.runfile, "epsilon-r", params.epsilon)

rule run_pt:
    """
    This rule should produce the structure and pseudo-trajectory as single files.
    """
    input:
        molecule1 = rules.copy_structures.output.m1,
        molecule2 = rules.copy_structures.output.m2,
        grid = rules.run_grid.output.full_array
    output:
        structure = f"{EXPERIMENT_FULL_PATH}structure.{STRUCTURE_EXTENSION}",
        trajectory = f"{EXPERIMENT_FULL_PATH}trajectory.{TRAJECTORY_EXTENSION}"
    params:
        cell_size_A = config["params_sqra"]["cell_size_A"]  # cubic box will be the output, this is size of the box in one dimension
    # benchmark:
    #     repeat(f"{EXPERIMENT_FULL_PATH}pt_benchmark.txt", NUM_REPEATS)
    run:
        from molgri.io import PtWriter
        my_writer = PtWriter(input.molecule1, input.molecule2, cell_size_A=params.cell_size_A, path_grid=input.grid)
        my_writer.write_full_pt(path_output_pt=output.trajectory,path_output_structure=output.structure)
        # if config["params_sqra"]["traj_as_dir"]:
        #     my_writer.write_full_pt_in_directory(paths_trajectory=output.trajectory, path_output_structure=output.structure)
        # else:
        #     my_writer.write_full_pt(path_output_pt=output.trajectory, path_output_structure=output.structure)

rule split_xyzfile:
    """
    If needed (e.g. for orca calculations) split a single file trajectory info a folder of single-structure files named 
    from 0000000000 to the total num of points.
    
    Warning, this is not a general tool for any .xyz files but specifically for my pseudotrajectory file.
    """
    input:
        trajectory = f"{EXPERIMENT_FULL_PATH}trajectory.{TRAJECTORY_EXTENSION}",
        num_atoms = rules.find_num_atoms.output.num_atoms
    output:
        trajectory_dict = directory(f"{EXPERIMENT_FULL_PATH}trajectory/")
    run:
        import subprocess
        with open(input.num_atoms) as f:
            line1 = f.readline()
            line2 = f.readline()
            num_atoms  = int(line1) + int(line2)
        num_lines = num_atoms + 2
        subprocess.run(f"split {input.trajectory} trajectory/ -l {num_lines} -d --suffix-length=10 --additional-suffix=.xyz",shell=True) # --'additional-suffix=.xyz'

rule gromacs_rerun:
    """
    This rule gets structure, trajectory, topology and gromacs run file as input, as output we are only interested in
    energies.
    """
    wildcard_constraints:
        ENERGY_PROGRAM  = "GROMACS"
    input:
        structure = rules.run_pt.output.structure,
        trajectory = rules.run_pt.output.trajectory,
        runfile = rules.copy_gromacs_input.output.runfile,
        topology = rules.copy_gromacs_input.output.dimer_topology,
        select_energy = rules.copy_gromacs_input.output.select_energy,
        force_field_stuff = rules.copy_gromacs_input.output.force_field_stuff
    shadow: "minimal"
    log:
        log = f"{EXPERIMENT_FULL_PATH}logging_gromacs.log"
    benchmark:
        repeat(f"{EXPERIMENT_FULL_PATH}gromacs_benchmark.txt", NUM_REPEATS)
    output:
        energy = f"{EXPERIMENT_FULL_PATH}energy.xvg",
    # use with arguments like path_structure path_trajectory path_topology path_default_files path_output_energy
    params:
        maxwarn = config["params_sqra"]["maxwarn"]
    shell:
        """
        initial_dir=$(pwd)
        cd $(dirname {input.structure})
        export PATH="/home/janjoswig/local/gromacs-2022/bin:$PATH"
        gmx22 grompp -f $(basename {input.runfile}) -c $(basename {input.structure}) -p $(basename {input.topology}) -o result.tpr  -maxwarn {params.maxwarn}
        gmx22 mdrun -s result.tpr -rerun $(basename {input.trajectory}) -g $(basename {log.log})
        gmx22 energy -f ener.edr -o $(basename {output.energy}) < $(basename {input.select_energy})
        cd "$initial_dir" || exit
        """


#rule touch_all_trajectory:
#    output:
#        touch([f"{EXPERIMENT_FULL_PATH}trajectory/{str(i).zfill(10)}.xyz" for i in range(NUM_GRID_POINTS)])

rule orca_SP_run:
    "Here we need to perform energy run and collection along the whole PT"
    wildcard_constraints:
        ENERGY_PROGRAM = "ORCA"
    input:
        xyz_file = f"{EXPERIMENT_FULL_PATH}trajectory/{{frame_num}}.{STRUCTURE_EXTENSION}"
    #shadow: "minimal"
    log:
        out_file = f"{EXPERIMENT_FULL_PATH}{ORCA_DIR}{{frame_num}}/orca.out" #
    benchmark:
        repeat(f"{EXPERIMENT_FULL_PATH}{ORCA_DIR}{{frame_num}}/orca_benchmark.txt",NUM_REPEATS) #{{frame_num}}/
    output:
        inp_file = f"{EXPERIMENT_FULL_PATH}{ORCA_DIR}{{frame_num}}/orca.inp" #{{frame_num}}/
    params:
        charge = config["params_dft"]["charge"],
        multiplicity = config["params_dft"]["multiplicity"],
    run:
        import subprocess

        dimer = QuantumMolecule(charge=params.charge, multiplicity=params.multiplicity, path_xyz=input.xyz_file)


        with open(output.inp_file, "w") as f:
            f.write(make_inp_file(dimer,QUANTUM_SETUP,geo_optimization=False))

        # run inp > out
        subprocess.run(f"/home/hanaz63/Programs/orca/orca_6_0_0_shared_openmpi416/orca {output.inp_file} > {log.out_file}", shell=True)

        # sometimes an overlapping structure will fail; that's okay as long as it is truly an exception
        assert_normal_finish(log.out_file, throw_error=False)


rule orca_collect_energies:
    """
    After energies for each point in trajectory have been calculated, combine them for
    """
    input:
        out_files = expand(f"{EXPERIMENT_FULL_PATH}{ORCA_DIR}{{frame_num}}/orca.out",
            frame_num = [str(i).zfill(10) for i in range(NUM_GRID_POINTS)]), #NUM_GRID_POINTS
        #benchmarks = expand(f"{EXPERIMENT_FULL_PATH}{ORCA_DIR}{{frame_num}}/orca_benchmark.txt",
        #   frame_num = [str(i).zfill(10) for i in range(NUM_GRID_POINTS)])
    output:
        energy = f"{EXPERIMENT_FULL_PATH}energy.csv"
    run:
        # note: since differences in energy are used, there is no need for ZPE-corrected energy
        # TODO: make sure the order remains the trajectory order
        # TODO: make sure to allow for control of the number of cycles and deal with failing structures
        read_important_stuff_into_csv(out_files_to_read=input.out_files, csv_file_to_write=output.energy,
            setup=QUANTUM_SETUP) #, benchmark_files_to_read=input.benchmarks

rule orca_total_and_average_time:
    input:
        energy = f"{EXPERIMENT_FULL_PATH}energy.csv"
    output:
        total_time = f"{EXPERIMENT_FULL_PATH}orca_time.txt"
    run:
        import pandas as pd

        my_df = pd.read_csv(input.energy)

        time_s = my_df["Time [s]"]
        print(time_s.describe())

        with open(output.total_time, "w") as f:
            f.write(f"Total time [s]: {time_s.sum():.2f}\n")
            f.write(f"Mean time [s]: {time_s.mean():.2f} +- {time_s.std():.2f}\n")
            f.write(f"Max time [s]: {time_s.max():.2f}\n")
            f.write(f"Min time [s]: {time_s.min():.2f}\n")
            f.write("--------------\n")
            f.write(f"Total time [h:m:s]: {pd.to_timedelta(time_s.sum(), unit='s')}\n")
            f.write(f"Mean time [h:m:s]: {pd.to_timedelta(time_s.mean(), unit='s')} +- {pd.to_timedelta(time_s.std(), unit='s')}\n")
            f.write(f"Max time [h:m:s]: {pd.to_timedelta(time_s.max(), unit='s')}\n")
            f.write(f"Min time [h:m:s]: {pd.to_timedelta(time_s.min(), unit='s')}\n")


rule lowest_e_structures:
    input:
        energy = rules.gromacs_rerun.output.energy
    output:
        list_structures = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/indices_lowest_E.csv"
    params:
        num = config["params_sqra"]["number_lowest_E_structures"]
    run:
        from molgri.space.utils import k_argmin_in_array
        from molgri.io import EnergyReader
        er = EnergyReader(input.energy).load_energy()
        energies2 = er["Coulomb (SR)"]
        energies5 = er["LJ (SR)"]
        energies6 = er["Potential"]
        energies7 = er["Disper. corr."]

        all_lowest_ind = k_argmin_in_array(energies6, k=params.num)
        all_lowest_E = energies6[all_lowest_ind]
        sort_index = np.argsort(all_lowest_E)
        sorted_indices = all_lowest_ind[sort_index]
        print("POTENTIAL")
        print(", ".join([str(x+1) for x in all_lowest_ind]))
        print(all_lowest_E)
        np.savetxt(output.list_structures, sorted_indices)


rule run_sqra:
    """
    As input we need: energies, adjacency, volume, borders, distances.
    As output we want to have the rate matrix.
    """
    input:
        energy = branch(
            lookup(dpath="energy_program", within=config),
            cases={
                "GROMACS": rules.gromacs_rerun.output.energy,
                "ORCA": rules.orca_collect_energies.output.energy
            }),
        distances_array = rules.run_grid.output.distances_array,
        borders_array = rules.run_grid.output.borders_array,
        volumes = rules.run_grid.output.volumes,
    output:
        rate_matrix = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/rate_matrix.npz",
        index_list = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/index_list.npy",
    benchmark:
        repeat(f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/rate_matrix_benchmark.txt", NUM_REPEATS)
    params:
        T=float(config["params_sqra"]["temperature_K"]),
        energy_type=config["params_sqra"]["energy_type"],
        m_h2o = float(config["params_sqra"]["mass_kg"]),
        tau = float(config["params_setup"]["tau_t"]),
        lower_lim = config["params_sqra"]["lower_lim_rate_matrix"],
        upper_lim = config["params_sqra"]["upper_lim_rate_matrix"],
    run:
        from molgri.io import EnergyReader
        from molgri.molecules.transitions import SQRA
        from scipy import sparse
        from scipy.constants import k as k_B

        tau = float(params.tau) * 1e-12 # now in s

        D = k_B * params.T *tau / params.m_h2o  # in m^2/s
        D*= 1e8  # now in A^2/ps
        print(f"Diffusion const D={D} ")

        # load input files
        all_volumes = np.load(input.volumes)
        all_surfaces = sparse.load_npz(input.borders_array)
        all_distances = sparse.load_npz(input.distances_array)



        # determine limits
        if params.lower_lim == "None":
            lower_limit = None
        else:
            lower_limit = float(params.lower_lim)
        if params.upper_lim == "None":
            upper_limit = None
        else:
            upper_limit = float(params.upper_lim)


        energies = EnergyReader(input.energy).load_single_energy_column(params.energy_type)
        # TODO: how to deal with NaNs?
        energies = np.nan_to_num(energies, nan=np.infty)
        print(energies)

        sqra = SQRA(energies=energies,volumes=all_volumes,distances=all_distances,surfaces=all_surfaces)

        print("created sqra")
        print("shape of ", D, params.T)

        rate_matrix = sqra.get_rate_matrix(D,params.T)
        print(np.max(rate_matrix.data), np.min(rate_matrix.data), np.max(-rate_matrix.data), np.min(-rate_matrix.data))
        rate_matrix, index_list = sqra.cut_and_merge(rate_matrix,T=params.T,lower_limit=lower_limit,
            upper_limit=upper_limit)
        # saving to file
        sparse.save_npz(output.rate_matrix,rate_matrix)
        np.save(output.index_list,np.array(index_list,dtype=object))



rule run_decomposition:
    """
    As output we want to have eigenvalues, eigenvectors. Es input we get a (sparse) rate matrix.
    """
    input:
        rate_matrix = rules.run_sqra.output.rate_matrix
    output:
        eigenvalues = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/eigenvalues.npy",
        eigenvectors = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/eigenvectors.npy",
    benchmark:
        repeat(f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/decomposition_benchmark.txt", NUM_REPEATS)
    params:
        tol=config["params_sqra"]["tol"],
        maxiter=config["params_sqra"]["maxiter"],
        sigma=config["params_sqra"]["sigma"],
        which=config["params_sqra"]["which"],
    run:
        from scipy import sparse
        from molgri.molecules.transitions import DecompositionTool

        # loading
        my_matrix = sparse.load_npz(input.rate_matrix)

        if params.sigma == "None":
            sigma = None
        else:
            sigma = float(params.sigma)
        # calculation
        dt = DecompositionTool(my_matrix)
        all_eigenval, all_eigenvec = dt.get_decomposition(tol=params.tol, maxiter=params.maxiter, which=params.which,
            sigma=sigma)

        # saving to file
        np.save(output.eigenvalues,np.array(all_eigenval))
        np.save(output.eigenvectors,np.array(all_eigenvec))


rule run_plot_everything_sqra:
    """
    Make a plot of eigenvalues
    """
    input:
        eigenvalues = rules.run_decomposition.output.eigenvalues,
        eigenvectors = rules.run_decomposition.output.eigenvectors,
    output:
        plot_eigenvectors=f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/eigenvectors.png",
        plot_eigenvalues=f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/eigenvalues.png",
        plot_its=report(f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/its.png", category="{experiment_id}"),
    run:
        from molgri.plotting.transition_plots import PlotlyTransitions

        pt = PlotlyTransitions(is_msm=False,path_eigenvalues=input.eigenvalues,path_eigenvectors=input.eigenvectors)
        # eigenvectors
        pt.plot_eigenvectors_flat()
        pt.save_to(output.plot_eigenvectors,height=800, width=400)
        # eigenvalues
        pt.plot_eigenvalues()
        pt.save_to(output.plot_eigenvalues)
        # # its for msm
        pt.plot_its_as_line()
        pt.save_to(output.plot_its)
        # we could also plot the heatmap of the matrix, but it's honestly not that useful and can become very large


rule compile_vmd_log:
    """
    Input are the saved eigenvectors. Output = a vmd log that can be used later with:

    vmd <gro file> <xtc file>
    play <vmdlog file>
    """
    input:
        structure = rules.run_pt.output.structure,
        trajectory = rules.run_pt.output.trajectory,
        eigenvectors = rules.run_decomposition.output.eigenvectors,
        index_list = rules.run_sqra.output.index_list,
        num_atoms = rules.find_num_atoms.output.num_atoms
    output:
        vmdlog = f"{EXPERIMENT_FULL_PATH}/eigenvectors_vmdlog",
        fig_tga = [f"{EXPERIMENT_FULL_PATH}/eigenvector{i}.tga" for i in range(config["params_sqra"]["num_eigenvec_to_plot"])],
        fig_png = [f"{EXPERIMENT_FULL_PATH}/eigenvector{i}.png" for i in range(config["params_sqra"]["num_eigenvec_to_plot"])]
    params:
        num_extremes=config["params_sqra"]["num_extremes_to_plot"],
        num_eigenvec=config["params_sqra"]["num_eigenvec_to_plot"]
    run:
        from molgri.plotting.create_vmdlog import VMDCreator, from_eigenvector_array_to_dominant_eigenvector_indices
        from workflow.snakemake_utils import  find_right_vmd_script

        # load eigenvectors
        eigenvectors = np.load(input.eigenvectors)

        # determine first and second index
        with open(input.num_atoms, "r") as f:
            num_first_mol = int(f.readline().strip())

        index_first = f"index < {num_first_mol}"
        index_second = f"index >= {num_first_mol}"

        if wildcards.experiment_type == "sqra_bpti_trypsine":
            is_protein=True
        else:
            is_protein=False

        index_list = np.load(input.index_list,allow_pickle=True)
        if not np.any(index_list):
            index_list = None
        else:
            index_list = list(index_list)
        if params.num_extremes == "None":
            params.num_extremes = None


        abs_e, pos_e, neg_e = from_eigenvector_array_to_dominant_eigenvector_indices(eigenvectors, index_list,
            n_eigenvectors=params.num_eigenvec, num_extremes=params.num_extremes)

        # now vmd creator
        vmd_creator = VMDCreator(index_first_molecule=index_first, index_second_molecule=index_second,
            is_protein=is_protein)
        vmd_creator.load_translation_rotation_script(find_right_vmd_script(EXPERIMENT_TYPE))
        vmd_creator.prepare_eigenvector_script(abs_e, pos_e, neg_e, plot_names=output.fig_tga)
        vmd_creator.write_text_to_file(output.vmdlog)

        # make all the figures
        shell("vmd  -dispdev text {input.structure} {input.trajectory} < {output.vmdlog}")
        for el_tga, el_png in zip(output.fig_tga, output.fig_png):
            shell("convert {el_tga} {el_png}")

rule print_its:
    input:
        eigenvalues = rules.run_decomposition.output.eigenvalues
    output:
        data = f"{PATH_EXPERIMENTS}{{experiment_type}}/{{experiment_id}}/{{grid_identifier}}/its.csv"
    run:
        import pandas as pd

        all_its = []
        eigenvals = np.load(input.eigenvalues)[1:]  # dropping the first one as it should be zero and cause issues
        all_its.append([-1 / (eigenval) for eigenval in eigenvals])
        my_df = pd.DataFrame(all_its, columns=[f"ITS {i} [ps]" for i in range(1, len(all_its[0])+1)])
        my_df.to_csv(output.data)



